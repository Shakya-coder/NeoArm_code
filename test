#!/usr/bin/env python3
"""
vision_lowlatency_auto.py
- Try direct /dev/video capture; fall back to launching rpicam-vid -> tcp stream.
- No GUI preview. Prints best detected object and category (hard/soft/unknown).
Usage:
    source .venv/bin/activate
    python3 vision_lowlatency_auto.py
"""
import os
import time
import threading
import subprocess
import signal
from pathlib import Path
import argparse

import cv2
from ultralytics import YOLO

# -----------------------
# Config
# -----------------------
ROOT = Path.home() / "NeoArm"
MODEL_PATH = ROOT / "vision" / "models" / "yolov8n.pt"
IMG_W, IMG_H = 640, 480
INFER_IMGSZ = 320
CONF = 0.30
TCP_STREAM = "tcp://127.0.0.1:8554"
RPICAM_CMD = [
    "/usr/bin/rpicam-vid",
    "-t", "0",
    "--inline",
    "--listen",
    "-o", "tcp://0.0.0.0:8554",
    "--width", str(IMG_W),
    "--height", str(IMG_H),
    "--framerate", "30"
]

# simple category mapping and rules
CATEGORY_MAP = {"bottle":"hard","cup":"hard","cell phone":"hard","phone":"hard",
                "apple":"soft","banana":"soft","orange":"soft","person":"unknown","hand":"unknown"}
def get_category(name):
    return CATEGORY_MAP.get(name.lower(), "unknown")

# -----------------------
# Frame buffer
# -----------------------
class FrameBuffer:
    def __init__(self):
        self.lock = threading.Lock()
        self.frame = None
        self.ts = 0.0
    def put(self, f):
        with self.lock:
            self.frame = f
            self.ts = time.time()
    def get(self):
        with self.lock:
            return self.frame, self.ts

# -----------------------
# Capture / inference
# -----------------------
def start_rpicam_stream():
    # start rpicam-vid as background process
    print("[vision] Starting rpicam-vid stream...")
    proc = subprocess.Popen(RPICAM_CMD, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, preexec_fn=os.setsid)
    time.sleep(0.8)  # give it a moment
    return proc

def open_capture_prefer_dev0():
    # Try direct device open first (no streaming)
    cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
    if cap is None or not cap.isOpened():
        if cap:
            try: cap.release()
            except: pass
        return None
    # Configure
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, IMG_W)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, IMG_H)
    cap.set(cv2.CAP_PROP_FPS, 30)
    return cap

def open_capture_tcp():
    # FFmpeg backend tends to work for tcp streams
    cap = cv2.VideoCapture(TCP_STREAM, cv2.CAP_FFMPEG)
    if cap is None or not cap.isOpened():
        if cap:
            try: cap.release()
            except: pass
        return None
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, IMG_W)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, IMG_H)
    cap.set(cv2.CAP_PROP_FPS, 30)
    return cap

def capture_thread_func(cap, buf: FrameBuffer, stop_event):
    while not stop_event.is_set():
        ret, frame = cap.read()
        if not ret or frame is None:
            time.sleep(0.005)
            continue
        # small resize to target capture size (if needed)
        if frame.shape[1] != IMG_W or frame.shape[0] != IMG_H:
            frame = cv2.resize(frame, (IMG_W, IMG_H))
        buf.put(frame)
    try:
        cap.release()
    except: pass

def inference_thread_func(model, buf: FrameBuffer, stop_event):
    names = getattr(model.model, "names", {})
    last_print = 0
    while not stop_event.is_set():
        frame, ts = buf.get()
        if frame is None:
            time.sleep(0.01)
            continue
        img = frame.copy()
        t0 = time.time()
        results = model.predict(source=[img], imgsz=INFER_IMGSZ, conf=CONF, max_det=5, verbose=False)
        t_elapsed = time.time() - t0
        detections = []
        if results and len(results) > 0:
            r = results[0]
            boxes = getattr(r, "boxes", [])
            for b in boxes:
                # conf and class extraction robustly
                try:
                    conf = float(b.conf[0]) if hasattr(b.conf, "__len__") else float(b.conf)
                except:
                    conf = float(getattr(b, "conf", 0.0))
                try:
                    cls_i = int(b.cls[0]) if hasattr(b.cls, "__len__") else int(b.cls)
                except:
                    cls_i = int(getattr(b, "cls", 0))
                name = names.get(cls_i, str(cls_i))
                category = get_category(name)
                detections.append((name, conf, category))
        if detections:
            best = max(detections, key=lambda x: x[1])
            name, conf, category = best
            print(f"[{time.strftime('%H:%M:%S')}] DETECTED: {name} conf={conf:.2f} cat={category} (inf_time={t_elapsed:.2f}s)")
        else:
            if time.time() - last_print > 1.0:
                print(f"[{time.strftime('%H:%M:%S')}] DETECTED: none (inf_time={t_elapsed:.2f}s)")
                last_print = time.time()
        # tiny sleep to avoid hogging CPU if model is very fast/slow
        time.sleep(0.005)

# -----------------------
# Main
# -----------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--force-tcp", action="store_true", help="Skip /dev/video0 probe and force rpicam-vid->tcp")
    args = parser.parse_args()

    if not MODEL_PATH.exists():
        print(f"[vision] ERROR: model not found at {MODEL_PATH}")
        return

    print("[vision] Loading model...", MODEL_PATH)
    model = YOLO(str(MODEL_PATH))
    print("[vision] Model loaded. Classes:", list(getattr(model.model, "names", {}).items())[:8])

    cap = None
    rpicam_proc = None

    try:
        if not args.force_tcp:
            cap = open_capture_prefer_dev0()
            if cap:
                print("[vision] Opened direct device /dev/video0")
        if cap is None:
            # start rpicam-vid and open tcp
            rpicam_proc = start_rpicam_stream()
            # Try a few times for stream to be ready
            ntries = 0
            while ntries < 6:
                cap = open_capture_tcp()
                if cap:
                    print("[vision] Connected to tcp stream", TCP_STREAM)
                    break
                time.sleep(0.5)
                ntries += 1
            if cap is None:
                raise RuntimeError("Failed to open capture (neither /dev/video0 nor tcp stream).")
        # start threads
        buf = FrameBuffer()
        stop_event = threading.Event()
        cap_thread = threading.Thread(target=capture_thread_func, args=(cap, buf, stop_event), daemon=True)
        inf_thread = threading.Thread(target=inference_thread_func, args=(model, buf, stop_event), daemon=True)
        cap_thread.start()
        inf_thread.start()

        print("[vision] Running (no preview). Ctrl+C to stop.")
        while True:
            time.sleep(0.5)

    except KeyboardInterrupt:
        print("[vision] Interrupted by user.")
    except Exception as e:
        print("[vision] Fatal error:", e)
    finally:
        try:
            stop_event.set()
        except:
            pass
        try:
            if rpicam_proc:
                os.killpg(os.getpgid(rpicam_proc.pid), signal.SIGTERM)
        except Exception:
            pass
        print("[vision] Exiting.")

if __name__ == "__main__":
    main()
