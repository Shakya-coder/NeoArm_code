"""
vision_opencv.py
Capture frames with OpenCV VideoCapture(0), run Ultralytics YOLOv8 model,
map detected class -> soft/hard/unknown, compute confidence%, bbox area norm,
score = confidence * bbox_area_norm, and print stop recommendation.

Usage (inside your venv, from project root):
    python3 vision/vision_opencv.py        # single-frame interactive (press SPACE)
    python3 vision/vision_opencv.py --live # continuous live mode (Ctrl+C to stop)
"""

import time, argparse, json, sys
from pathlib import Path
import cv2
import numpy as np
from ultralytics import YOLO

# Config
MODEL_PATH = "vision/models/yolov8n.pt"   # adjust if different
FRAME_W = 640
FRAME_H = 480
DEFAULT_CONF_THRESH = 0.35
STOP_SCORE_THRESH = 0.45

CLASS_TO_CATEGORY = {
    "bottle": "hard",
    "cell phone": "hard", "phone": "hard", "mobile phone": "hard",
    "sports ball": "hard",  # cricket ball -> sports ball
    "apple": "soft", "banana": "soft", "orange": "soft",
    "person": "unknown", "hand": "unknown",
    # add more mappings as you like
}

def get_class_category(name):
    k = name.lower()
    if name in CLASS_TO_CATEGORY:
        return CLASS_TO_CATEGORY[name]
    if k in CLASS_TO_CATEGORY:
        return CLASS_TO_CATEGORY[k]
    if "phone" in k or "mobile" in k:
        return "hard"
    for fruit in ("apple","banana","orange","mango","grape","pineapple"):
        if fruit in k: return "soft"
    if "person" in k or "hand" in k: return "unknown"
    return "unknown"

def open_capture(device_index=0, w=FRAME_W, h=FRAME_H):
    cap = cv2.VideoCapture(device_index, cv2.CAP_V4L2)  # use V4L2 backend if available
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, w)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, h)
    # cap.set(cv2.CAP_PROP_FPS, 30)
    time.sleep(0.2)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video device {device_index}")
    return cap

def classify_frame(model, frame, model_names=None, default_conf=DEFAULT_CONF_THRESH, stop_score=STOP_SCORE_THRESH):
    # model.predict accepts BGR or RGB; Ultralytics expects numpy arrays (it handles conversion)
    results = model.predict(source=[frame], imgsz=max(frame.shape[1], frame.shape[0]), verbose=False, conf=0.01, max_det=10)
    if not results:
        return None
    r = results[0]
    boxes = getattr(r, "boxes", None)
    if boxes is None or len(boxes) == 0:
        return None

    # pick highest-confidence detection
    best_idx = 0
    best_conf = -1.0
    for i, box in enumerate(boxes):
        conf = float(box.conf[0]) if hasattr(box, "conf") else float(box.conf)
        if conf > best_conf:
            best_conf = conf
            best_idx = i

    b = boxes[best_idx]
    conf = float(b.conf[0]) if hasattr(b, "conf") else float(b.conf)
    cls_idx = int(b.cls[0]) if hasattr(b, "cls") else int(b.cls)
    xyxy = b.xyxy[0].cpu().numpy() if hasattr(b, "xyxy") else np.array([0,0,0,0])
    x1, y1, x2, y2 = [float(v) for v in xyxy]
    w = max(0.0, x2 - x1)
    h = max(0.0, y2 - y1)
    frame_area = frame.shape[0] * frame.shape[1]
    bbox_area = w * h
    bbox_area_norm = min(1.0, bbox_area / frame_area) if frame_area > 0 else 0.0
    score = conf * bbox_area_norm

    class_name = str(cls_idx)
    if model_names and 0 <= cls_idx < len(model_names):
        class_name = model_names[cls_idx]

    category = get_class_category(class_name)
    meets_conf = conf >= default_conf
    meets_score = score >= stop_score
    stop_recommendation = bool(meets_conf and meets_score)

    return {
        "class": class_name,
        "confidence": round(conf, 4),
        "accuracy_pct": round(conf * 100.0, 2),
        "category": category,
        "bbox": [round(x1,2), round(y1,2), round(w,2), round(h,2)],
        "bbox_area_norm": round(bbox_area_norm,4),
        "score": round(score,4),
        "stop_recommendation": stop_recommendation
    }

def run_demo(live=False):
    model_file = Path(MODEL_PATH)
    if not model_file.exists():
        print(f"[WARN] Model not found at {MODEL_PATH}. If you prefer caching model, the Ultralytics loader will auto-download it.")
    print("[vision] Loading model...")
    model = YOLO(str(model_file))
    model_names = getattr(model.model, "names", None)
    print("[vision] Model loaded. Classes:", list(model_names.items())[:8] if model_names else "n/a")

    cap = open_capture()
    try:
        if not live:
            print("Interactive mode: press SPACE to classify frame, Q to quit.")
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("[vision] Frame read failed.")
                    break
                disp = cv2.resize(frame, (640, 360))
                cv2.imshow("Preview - press SPACE to classify", disp)
                k = cv2.waitKey(1) & 0xFF
                if k == ord(' '):
                    print("[vision] Running classification...")
                    evt = classify_frame(model, frame, model_names=model_names)
                    print(json.dumps(evt, indent=2))
                elif k == ord('q'):
                    break
            cv2.destroyAllWindows()
        else:
            print("Live mode: running continuous classification. Ctrl+C to stop.")
            while True:
                ret, frame = cap.read()
                if not ret:
                    time.sleep(0.1)
                    continue
                evt = classify_frame(model, frame, model_names=model_names)
                if evt:
                    print(f"[{time.strftime('%H:%M:%S')}] {evt['class']} ({evt['accuracy_pct']}%)  category={evt['category']} score={evt['score']} stop={evt['stop_recommendation']}")
                else:
                    print(f"[{time.strftime('%H:%M:%S')}] No detection")
                # adjust sleep if CPU overloaded
                time.sleep(0.2)
    except KeyboardInterrupt:
        print("Stopped by user.")
    finally:
        try: cap.release()
        except: pass
        cv2.destroyAllWindows()

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--live", action="store_true")
    args = p.parse_args()
    run_demo(live=args.live)
